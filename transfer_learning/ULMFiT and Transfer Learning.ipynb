{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Transfer Learning\n",
    "While one of the biggest advancements in deep learning for image processing has been transfer learning. There are known open source models such as VGGNet, ResNet, Inception, and others that have been trained on ImageNet data.\n",
    "\n",
    "VGGNet\n",
    "ResNet\n",
    "Inception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Transfer Learning?\n",
    "\n",
    "Take a neural network.\n",
    "Train it and export the model.\n",
    "\n",
    "Import part or all of the model architecture into a new model!\n",
    "Use parts of it to build a new model!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Transfer Learning?\n",
    "\n",
    "* Save training time 1 Month --> 1 Day\n",
    "* Reuse models\n",
    "* Train on a smaller data set for a specific task\n",
    "* Don't need to reinvent the wheel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "We previously discussed Word Embeddings. In the past, with NLP people have performed transfer learning with just the first Embedding layer. One common one is:\n",
    "\n",
    "GloVe: Global Vectors for Word Representation\n",
    "https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "However, beyond the embedding layer, there have not been many successful shared examples of Transfer Learning in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ULMFiT\n",
    "Jeremy Howard and Sebastian Ruder from Fast.AI\n",
    "\n",
    "Why ULMFiT\n",
    "* Faster time retraining for classification task\n",
    "* Better performance\n",
    "* Smaller amount of labeled training data required\n",
    "\n",
    "http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html\n",
    "https://github.com/fastai\n",
    "https://arxiv.org/abs/1801.06146"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: General Language Model Pre-training\n",
    "First they train the model on a large corpus such as the Wikitext-103 data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Language Model Fine Tuning\n",
    "Then they fine-tune the language model on the target data set, so they can get the characteristics of the data set \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Classifier Fine-Tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
