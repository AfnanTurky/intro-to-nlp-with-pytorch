{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/prananth/intro-to-nlp-with-pytorch/Sarcasm_Detection/fleiss_kappa\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Entropy\n",
    "\n",
    "The Max Entropy classifier is a probabilistic classifier which belongs to the class of exponential models. \n",
    "\n",
    "The Max Entropy does not assume that the features are conditionally independent of each other. \n",
    "\n",
    "The MaxEnt is based on the Principle of Maximum Entropy and from all the models that fit our training data, selects the one which has the largest entropy. The Max Entropy classifier can be used to solve a large variety of text classification problems such as language detection, topic classification, sentiment analysis and more.\n",
    "\n",
    "![img](dogvsfriedchicken.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure,\n",
    "                                          log_likelihood, approxrand)\n",
    "from nltk import precision\n",
    "import random\n",
    "from nltk import classify\n",
    "from nltk.classify import MaxentClassifier\n",
    "from nltk.classify.megam import call_megam, write_megam_file, parse_megam_weights\n",
    "from nltk.corpus import names\n",
    "import collections,re\n",
    "import csv\n",
    "import json\n",
    "\n",
    "train_data = sys.argv[1]\n",
    "test_data = sys.argv[2]\n",
    "\n",
    "nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
    "\n",
    "all_features = [\"words\",\"length\",\"pos\",\"interjection\",\"question\"]\n",
    "metrics = {\n",
    "\n",
    "          }\n",
    "def feature_set_generator(original_tweet,text,hashtags,users,length,label, include_list):\n",
    "    features = {}\n",
    "    words = text.split()\n",
    "\n",
    "    if not include_list:\n",
    "        include_list = all_features\n",
    "\n",
    "    # Bag of words\n",
    "    if(\"words\" in include_list):\n",
    "        features[\"words\"] = tuple((word,True) for word in words)\n",
    "\n",
    "    # Length\n",
    "    if(\"length\" in include_list):\n",
    "        features[\"length\"] = length\n",
    "\n",
    "    # Part of speech tagging\n",
    "    pos = nltk.word_tokenize(text)\n",
    "    if(\"pos\" in include_list):\n",
    "        set_of_pos_tags = nltk.pos_tag(pos)\n",
    "        features[\"pos\"] = tuple(t for t in set_of_pos_tags)\n",
    "\n",
    "\n",
    "    # Interjections - SUBSTANTIAL INCREASE IN ACCURACY\n",
    "    if(\"interjection\" in include_list):\n",
    "        set_of_pos_tags = nltk.pos_tag(pos)\n",
    "        interjection_tags = 0\n",
    "        for tag in set_of_pos_tags:\n",
    "            if tag == \"UH\":\n",
    "                interjection_tags += 1\n",
    "        features[\"interjection\"] = interjection_tags\n",
    "\n",
    "    if(\"question\" in include_list):\n",
    "        question_count = 0\n",
    "        for text in words:\n",
    "            if \"?\" in text:\n",
    "                question_count += 1\n",
    "        features[\"question\"] = question_count\n",
    "\n",
    "    return features\n",
    "\n",
    "def me_classifier(exclude_list):\n",
    "    me_classifier = 0\n",
    "\n",
    "    with open(train_data, 'r',encoding='utf-8', errors='ignore') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        feature_set = [(feature_set_generator(original_tweet,text,hashtags,users,length,label,exclude_list),label) for original_tweet,text,hashtags,users,length,label in reader]\n",
    "        #print(feature_set)\n",
    "        me_classifier = MaxentClassifier.train(feature_set,\"megam\")\n",
    "\n",
    "    accuracy = 0.0\n",
    "    with open(test_data,'r',encoding='utf-8', errors='ignore') as testcsvfile:\n",
    "        test_reader = csv.reader(testcsvfile)\n",
    "        test_feature_set = [(feature_set_generator(original_tweet,text,hashtags,users,length,label,exclude_list),label) for original_tweet,text,hashtags,users,length,label in test_reader]\n",
    "        accuracy = classify.accuracy(me_classifier, test_feature_set)\n",
    "\n",
    "    classified = collections.defaultdict(set)\n",
    "    observed = collections.defaultdict(set)\n",
    "    i=1\n",
    "    with open(test_data,'r',encoding='utf-8', errors='ignore') as testcsvfile:\n",
    "        test_reader = csv.reader(testcsvfile)\n",
    "        for original_tweet,text,hashtags,users,length,label in test_reader:\n",
    "            observed[label].add(i)\n",
    "            classified[me_classifier.classify(feature_set_generator(original_tweet,text,hashtags,users,length,label,exclude_list))].add(i)\n",
    "            i+=1\n",
    "\n",
    "    return accuracy,precision(observed[\"S\"], classified[\"S\"]),recall(observed['S'], classified['S']),\\\n",
    "           f_measure(observed['S'], classified['S']),precision(observed['NS'], classified['NS']),recall(observed['S'], classified['NS']),f_measure(observed['S'], classified['NS'])\n",
    "\n",
    "\n",
    "def print_stats(a,ps,rs,fs,pns,rns,fns):\n",
    "    print()\n",
    "    print(\"****************** MAX ENTROPY STATISTICS******************************\")\n",
    "    print('Accuracy:', a)\n",
    "    print('Sarcasm precision:', ps)\n",
    "    print('Sarcasm recall:', rs)\n",
    "    print('Sarcasm F-measure:', fs)\n",
    "    print('Not Sarcasm precision:',pns)\n",
    "    print('Not Sarcasm recall:', rns)\n",
    "    print('Not Sarcasm F-measure:', fns)\n",
    "    print(\"***********************************************************************\")\n",
    "\n",
    "\n",
    "def prepare_dict(dict,a,ps,rs,fs,pns,rns,fns):\n",
    "    dict = {}\n",
    "    dict[\"title\"] = \"Maximum Entropy with all features\"\n",
    "    dict[\"accuracy\"] = a\n",
    "    dict[\"sarcasm_precision\"] = ps\n",
    "    dict[\"sarcasm_recall\"] = rs\n",
    "    dict[\"sarcasm_f_measure\"] = fs\n",
    "    dict[\"not_sarcasm_precision\"] = pns\n",
    "    dict[\"not_sarcasm_recall\"] = rns\n",
    "    dict[\"not_sarcasm_f_measure\"] = fns\n",
    "    return dict\n",
    "\n",
    "a,ps,rs,fs,pns,rns,fns = me_classifier([])\n",
    "max_ent_with_all_features = {}\n",
    "metrics[\"max_ent_with_all_features\"]=prepare_dict(max_ent_with_all_features,a,ps,rs,fs,pns,rns,fns)\n",
    "print_stats(a,ps,rs,fs,pns,rns,fns)\n",
    "\n",
    "a,ps,rs,fs,pns,rns,fns = me_classifier([\"pos\"])\n",
    "max_ent_with_only_pos = {}\n",
    "metrics[\"max_ent_with_only_pos\"]=prepare_dict(max_ent_with_only_pos,a,ps,rs,fs,pns,rns,fns)\n",
    "print_stats(a,ps,rs,fs,pns,rns,fns)\n",
    "\n",
    "a,ps,rs,fs,pns,rns,fns = me_classifier([\"polarity\"])\n",
    "max_ent_with_only_polarity = {}\n",
    "metrics[\"max_ent_with_only_polarity\"]=prepare_dict(max_ent_with_only_polarity,a,ps,rs,fs,pns,rns,fns)\n",
    "print_stats(a,ps,rs,fs,pns,rns,fns)\n",
    "\n",
    "a,ps,rs,fs,pns,rns,fns = me_classifier([\"interjection\"])\n",
    "max_ent_with_only_interjection = {}\n",
    "metrics[\"max_ent_with_only_interjection\"]=prepare_dict(max_ent_with_only_interjection,a,ps,rs,fs,pns,rns,fns)\n",
    "print_stats(a,ps,rs,fs,pns,rns,fns)\n",
    "\n",
    "a,ps,rs,fs,pns,rns,fns = me_classifier([\"words\",\"length\",\"hashtag\",\"pos\",\"interjection\",\"polarity\"])\n",
    "max_ent_without_onamatopoeia_and_question = {}\n",
    "metrics[\"max_ent_without_onamatopoeia_and_question\"]=prepare_dict(max_ent_without_onamatopoeia_and_question,a,ps,rs,fs,pns,rns,fns)\n",
    "print_stats(a,ps,rs,fs,pns,rns,fns)\n",
    "\n",
    "a,ps,rs,fs,pns,rns,fns = me_classifier([\"question\",\"length\",\"interjection\"])\n",
    "max_ent_with_question_length_interjection = {}\n",
    "metrics[\"max_ent_with_question_length_interjection\"]=prepare_dict(max_ent_with_question_length_interjection,a,ps,rs,fs,pns,rns,fns)\n",
    "print_stats(a,ps,rs,fs,pns,rns,fns)\n",
    "\n",
    "json_data = json.dumps(metrics)\n",
    "output_json = open('metrics.json','w')\n",
    "output_json.write(json_data)\n",
    "output_json.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
