{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"https://raw.githubusercontent.com/PythonWorkshop/intro-to-nlp-with-pytorch/master/images/logo.png\" align=\"left\" width=\"25%\">\n",
    "\n",
    "# Introduction\n",
    "\n",
    "## Who are we\n",
    "\n",
    "* Kendall Chuang\n",
    "* David Clark\n",
    "* Anna Bethke\n",
    "* Mehrdad Yazdani\n",
    "* Micheleen Harris\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Questions about you\n",
    "* Python background?\n",
    "* Jupyter background?\n",
    "* Deep Learning background?\n",
    "* NLP background?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why sequences are important\n",
    "\n",
    "<img src=\"../images/why_sequences.png\" alt=\"why sequences\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a Recurrent Neural Network (RNN)\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/PythonWorkshop/intro-to-nlp-with-pytorch/master/images/rnn_inner_workings.png\" alt=\"inside rnn\" width=\"50%\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* x = input embedding for a word (vector)\n",
    "* h = hidden (or activation) state (vector)\n",
    "* tanh = hyperbolic tangent activation function/layer\n",
    "* y = output tag (not shown because we can have different schemes)\n",
    "\n",
    "**A Long Short-Term Memory (LSTM) network is a subclass of RNNs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What types are RNNs are there? TODO: diagram\n",
    "\n",
    "* Many-to-many - e.g. find names with named entity recognition (NER)\n",
    "* Many-to-one - e.g. sentiment analysis\n",
    "* One-to-many - e.g. music generation\n",
    "* Another many-to-many - e.g. machine translation\n",
    "* One-to-one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Want to create a few tensors right now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# First tensor calculation! # TODO a diagram\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# PyTorch takes care of gradient differentiation for you with something called \"autograd\"!\n",
    "# Makes backwards propagation super, duper simple (we don't have to worry about it!)\n",
    "# But you must tell the leafs that they require this tracking\n",
    "\n",
    "# Some random data (2D and 1D vectors)\n",
    "x = torch.randn(5, 5, requires_grad=True)\n",
    "b = torch.randn(5, requires_grad=True)\n",
    "\n",
    "# Just a one-element tensor\n",
    "w = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# Do some multiplication\n",
    "y = w * x\n",
    "\n",
    "# Do some addition\n",
    "z = y + b\n",
    "\n",
    "# Let's trace back the operations\n",
    "print(z.grad_fn)\n",
    "print(y.grad_fn)\n",
    "print(x.grad_fn)\n",
    "\n",
    "# Why do you think x's auto gradient differentiation function is \"None\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "1.  [RNN video \"RNN1. Why sequence models?\"](https://www.youtube.com/watch?v=5Vl-bK7tfD8&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=1) by Andrew Ng\n",
    "2.  [Getting Started with PyTorch Part 1: Understanding how Automatic Differentiation works](https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec)\n",
    "3.  [Introduction to PyTorch fro pytorch.org](https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html#sphx-glr-beginner-nlp-pytorch-tutorial-py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
