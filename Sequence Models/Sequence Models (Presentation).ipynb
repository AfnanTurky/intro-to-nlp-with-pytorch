{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Models\n",
    "Tutorial link: [Sequence Models and Long-Short Term Memory Networks](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html)\n",
    "\n",
    "Original author: Robert Guthrie.\n",
    "\n",
    "In this tutorial, you will learn about LSTM neural networks and see an example of how they can be used to recognize parts of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11165afd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import jdc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import Image\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "### RNNs\n",
    "* Difficulty learning widely separated relationships in long sequences.\n",
    "### What is an LSTM?\n",
    "* Long Short-Term Memory.\n",
    "* Able to learn long term dependencies in a sequence.\n",
    "* Uses a cell state and gates to remember relationships.\n",
    "    \n",
    "**Example:** _The humidity is very high. Today it is going to_ _**rain**_. A regular RNN might not do a good job predicting the word **rain**.\n",
    "\n",
    "### Rolled Out LSTM Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/PythonWorkshop/intro-to-nlp-with-pytorch/master/images/lstm_flow.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/PythonWorkshop/intro-to-nlp-with-pytorch/master/images/lstm_inner_workings.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Components\n",
    "An LSTM uses three gates:\n",
    "* Forget Gate\n",
    "* Input Gate\n",
    "* Output Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forget Gate\n",
    "* Decide what information to remove from cell state.\n",
    "* Sigmoid layer.\n",
    "\n",
    "**Example Continued:** \n",
    "* Network receives as input low humidity.\n",
    "* Network adjusts the rain likelihood for today to low. \n",
    "* The forget gate then removes the current state of rain likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/LSTM3-focus-f.png\" width=80%>\n",
    "<div class=\"row\" style=\"font-size: 10px\">\n",
    "    <div class=\"col-md-12\">\n",
    "        <p><a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Understanding LSTM Networks</a>, colah's blog, August 27, 2015\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Gate.\n",
    "* Decide what information to store in cell state.\n",
    "* Decide what to update.\n",
    "    * Sigmoid layer. \n",
    "* Create candidate values for cell state.\n",
    "    * Tanh layer.\n",
    "    \n",
    "**Example Continued:**\n",
    "* replace the previous rain likelihood with the new likelihood for dry weather."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/LSTM3-focus-i.png\" width=80%>\n",
    "<div class=\"row\" style=\"font-size: 10px\">\n",
    "    <div class=\"col-md-12\">\n",
    "        <p><a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Understanding LSTM Networks</a>, colah's blog, August 27, 2015\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update Cell State\n",
    "* Remove forgotten information.\n",
    "    * Multiply $f_t$ by old cell state.\n",
    "\n",
    "* Add new candidate values scaled by their importance.\n",
    "    * Add $i_t\\ast\\tilde{C}_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/LSTM3-focus-C.png\" width=80%>\n",
    "<div class=\"row\" style=\"font-size: 10px\">\n",
    "    <div class=\"col-md-12\">\n",
    "        <p><a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Understanding LSTM Networks</a>, colah's blog, August 27, 2015\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Gate\n",
    "* Return filtered version of the cell state.\n",
    "\n",
    "**Example Continued:**\n",
    "* Keep track of humidity magnitude to help the network decide whether to predict a large storm or just light rain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/LSTM3-focus-o.png\" width=80%>\n",
    "<div class=\"row\" style=\"font-size: 10px\">\n",
    "    <div class=\"col-md-12\">\n",
    "        <p><a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Understanding LSTM Networks</a>, colah's blog, August 27, 2015\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Very Simple Example 1\n",
    "* Create an LSTM in PyTorch using a **`for`** loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_1: tensor([[[0.1286]]], grad_fn=<StackBackward>)\n",
      "h_1: (tensor([[[0.1286]]], grad_fn=<StackBackward>), tensor([[[0.5273]]], grad_fn=<StackBackward>))\n",
      "\n",
      "x_2: tensor([[[0.1395]]], grad_fn=<StackBackward>)\n",
      "h_2: (tensor([[[0.1395]]], grad_fn=<StackBackward>), tensor([[[0.4831]]], grad_fn=<StackBackward>))\n",
      "\n",
      "x_3: tensor([[[0.1322]]], grad_fn=<StackBackward>)\n",
      "h_3: (tensor([[[0.1322]]], grad_fn=<StackBackward>), tensor([[[0.5156]]], grad_fn=<StackBackward>))\n",
      "\n",
      "x_4: tensor([[[0.1449]]], grad_fn=<StackBackward>)\n",
      "h_4: (tensor([[[0.1449]]], grad_fn=<StackBackward>), tensor([[[0.4218]]], grad_fn=<StackBackward>))\n",
      "\n",
      "x_5: tensor([[[0.1443]]], grad_fn=<StackBackward>)\n",
      "h_5: (tensor([[[0.1443]]], grad_fn=<StackBackward>), tensor([[[0.4398]]], grad_fn=<StackBackward>))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define LSTM architecture\n",
    "sequence_len = 5  # The length of the sequence\n",
    "input_size = 1  # Number of input features per time step\n",
    "hidden_size = 1  # Number of LSTM blocks per layer of the RNN\n",
    "batch_size = 1  # The batch size\n",
    "output_size = hidden_size  \n",
    "lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size)\n",
    "\n",
    "# Create fake inputs for the LSTM\n",
    "inputs = torch.randn(sequence_len, batch_size, input_size)\n",
    "\n",
    "# Initialize the hidden state and cell states.\n",
    "hidden_0 = torch.randn(1, batch_size, hidden_size)\n",
    "cell_0 = torch.randn(1, batch_size, hidden_size)\n",
    "\n",
    "# Step through the LSTM as it takes in the input sequence\n",
    "for i, in_value in enumerate(inputs):\n",
    "    # Step through the sequence one element at a time.\n",
    "    # After each time step, hidden contains the hidden state.\n",
    "    out, hidden_out = lstm(in_value.view(1, 1, -1), (hidden_0, cell_0))\n",
    "    print('x_{}: {}'.format(i+1, out))\n",
    "    print('h_{}: {}'.format(i+1, hidden_out))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, **`x`** is the output and **`h`** is the value of the hidden and cell states at each step in the sequence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Very Simple Example 2\n",
    "* Create an LSTM in PyTorch using **`cat`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM architecture\n",
    "sequence_len = 5  # The length of the sequence\n",
    "input_size = 1  # Number of input features per time step\n",
    "hidden_size = 1  # Number of LSTM blocks per layer of the RNN\n",
    "batch_size = 1  # The batch size\n",
    "output_size = hidden_size  \n",
    "lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size)\n",
    "\n",
    "# Create the inputs for the LSTM\n",
    "inputs = [torch.randn(batch_size, input_size) for _ in range(sequence_len)]\n",
    "\n",
    "# Concatenate the inputs so that they are a tensor\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "\n",
    "# Initialize the hidden state and cell states.\n",
    "hidden_0 = torch.randn(1, batch_size, hidden_size)\n",
    "cell_0 = torch.randn(1, batch_size, hidden_size)\n",
    "\n",
    "out, hidden = lstm(inputs, (hidden_0, cell_0))  # out = all states, hidden = last state and last cell state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: tensor([[[-0.1616]],\n",
      "\n",
      "        [[-0.1378]],\n",
      "\n",
      "        [[ 0.0745]],\n",
      "\n",
      "        [[-0.2183]],\n",
      "\n",
      "        [[-0.2227]]], grad_fn=<StackBackward>)\n",
      "last hidden and cell states: (tensor([[[-0.2227]]], grad_fn=<StackBackward>), tensor([[[-0.9171]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "print('out: {}'.format(out))\n",
    "print('last hidden and cell states: {}'.format(hidden))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: An LSTM for Part-of-Speech Tagging\n",
    "* Predict parts-of-speach in a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data:\n",
    "* Training data is a list of list pairs.\n",
    "    * First list is a sentence.\n",
    "    * Second list are the parts-of-speech tags for each word in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    (\"The dog ate the apple.\".split(), [\"Determiner\", \"Noun\", \"Verb\", \"Determiner\", \"Noun\"]),\n",
    "    (\"Everybody read that book.\".split(), [\"Noun\", \"Verb\", \"Determiner\", \"Noun\"])\n",
    "]\n",
    "training_sentences = [training_data[x][0] for x in range(len(training_data))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using dictionaries to convert words to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_to_ix: {'read': 6, 'The': 0, 'that': 7, 'ate': 2, 'book.': 8, 'apple.': 4, 'dog': 1, 'Everybody': 5, 'the': 3}\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print('word_to_ix: {}'.format(word_to_ix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map the parts-of-speech tags to integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tags to integers\n",
    "tag_to_ix = {\"Determiner\": 0, \"Noun\": 1, \"Verb\": 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map the integers back to parts-of-speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integers to tags\n",
    "ix_to_tag = {0: \"Determiner\", 1: \"Noun\", 2: \"Verb\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "LEARNING_RATE = 0.1\n",
    "NUM_EPOCHS = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "* **`LSTMTagger`** class.\n",
    "    * Inherits **`nn.Module`** from PyTorch.\n",
    "    * Inputs:\n",
    "        * Embedding dimension.\n",
    "        * Number of hidden dimensions.\n",
    "        * Vocabulary size.\n",
    "        * Tag set size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM: Inputs are embeddings, outputs are hidden states\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # Linear layer maps hidden space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to initialize the hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to LSTMTagger\n",
    "def init_hidden(self):\n",
    "    \"\"\"\n",
    "    Initialize the hidden state. The axes correspond to (num_layers, minibatch_size, hidden_dim).\n",
    "    \"\"\"\n",
    "    return (torch.zeros(1, 1, self.hidden_dim),\n",
    "            torch.zeros(1, 1, self.hidden_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to make a forward pass through the recurrent LSTM network. It will return the predict tag values given an input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to LSTMTagger\n",
    "def forward(self, sentence):\n",
    "    \"\"\"\n",
    "    Make a forward pass through the LSTM.\n",
    "    \n",
    "    :param sentence: The input sentence.\n",
    "    :type sentence: list\n",
    "    :return: A Tensor of tag scores.\n",
    "    :rtype: Tensor\n",
    "    \"\"\"\n",
    "    embeds = self.word_embeddings(sentence)\n",
    "    lstm_out, self.hidden = self.lstm(\n",
    "        embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "    tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "    tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "    return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function\n",
    "Map either words or tags to integers, using the previously defined dictionaries (**`tag_to_ix`**, **`ix_to_tag`**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    \"\"\"\n",
    "    Convert words or tags to intigers and return a Pytorch tensor.\n",
    "    :param seq: Sequence of words.\n",
    "    :type seq: list\n",
    "    :param to_ix: Dictionary mapping words or tags to intigers.\n",
    "    :return: A Pytorch tensor of indices.\n",
    "    :rtype: Tensor\n",
    "    \"\"\"\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the LSTM Pytorch model using the hyperparameters defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function. In this case, we will be using a negative log likelihood function, which is useful in classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative Log Likelihood\n",
    "We can illustrate negative log likelihood in the following diagram:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/PythonWorkshop/intro-to-nlp-with-pytorch/master/images/nll_loss.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the model using stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the model before any training has been done and store the scores to a **`list`**. We will then compare these scores with the scores after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_initial_probabilities = []\n",
    "store_initial_predictions = []\n",
    "with torch.no_grad():\n",
    "    for sentence in training_sentences:\n",
    "        inputs = prepare_sequence(sentence, word_to_ix)\n",
    "        tag_scores = model(inputs)\n",
    "        tag_probabilities = tag_scores.exp()\n",
    "        max_values, max_indices = torch.max(tag_probabilities, 1)\n",
    "        initial_prediction = [ix_to_tag[x] for x in max_indices.numpy()]\n",
    "        store_initial_predictions.append(initial_prediction)\n",
    "        store_initial_probabilities.append(tag_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    for sentence, tags in training_data:\n",
    "        # Set gradients equal to zero after each intance\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Initialize hidden state of LSTM after each intance\n",
    "        model.hidden = model.init_hidden()\n",
    "        \n",
    "        # Turn inputs into tensors of word indices\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "        \n",
    "        # Run forward pass\n",
    "        tag_scores = model(sentence_in)\n",
    "        \n",
    "        # Compute the loss, gradients, and update the parameters\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        \n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update model parameters\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has now finished training. Let's print out some statistics to show how well the model training performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:\n",
      " - initial probabilities: tensor([[0.4666, 0.2908, 0.2426],\n",
      "        [0.4649, 0.2811, 0.2540],\n",
      "        [0.4950, 0.2623, 0.2427],\n",
      "        [0.4749, 0.2906, 0.2345],\n",
      "        [0.4839, 0.2764, 0.2398]])\n",
      " - sentence: The dog ate the apple.\n",
      " - predicition: ['Determiner', 'Determiner', 'Determiner', 'Determiner', 'Determiner']\n",
      "After training:\n",
      " - final probabilities: tensor([[0.6031, 0.2518, 0.1451],\n",
      "        [0.0267, 0.9718, 0.0014],\n",
      "        [0.0592, 0.0161, 0.9246],\n",
      "        [0.9479, 0.0264, 0.0257],\n",
      "        [0.0104, 0.9842, 0.0054]])\n",
      " - sentence: The dog ate the apple.\n",
      " - prediction: ['Determiner', 'Noun', 'Verb', 'Determiner', 'Noun']\n",
      "\n",
      "Before training:\n",
      " - initial probabilities: tensor([[0.4878, 0.2757, 0.2366],\n",
      "        [0.5013, 0.2558, 0.2429],\n",
      "        [0.4658, 0.2717, 0.2625],\n",
      "        [0.4935, 0.2628, 0.2437]])\n",
      " - sentence: Everybody read that book.\n",
      " - predicition: ['Determiner', 'Determiner', 'Determiner', 'Determiner']\n",
      "After training:\n",
      " - final probabilities: tensor([[0.0037, 0.9917, 0.0046],\n",
      "        [0.0658, 0.0127, 0.9214],\n",
      "        [0.9636, 0.0201, 0.0163],\n",
      "        [0.0049, 0.9918, 0.0033]])\n",
      " - sentence: Everybody read that book.\n",
      " - prediction: ['Noun', 'Verb', 'Determiner', 'Noun']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out the scores after training the model\n",
    "store_initial_probabilities.reverse()\n",
    "store_initial_predictions.reverse()\n",
    "with torch.no_grad():\n",
    "    for sentence in training_sentences:\n",
    "        inputs = prepare_sequence(sentence, word_to_ix)\n",
    "        tag_scores = model(inputs)\n",
    "        tag_probabilities = tag_scores.exp()\n",
    "        max_values, max_indices = torch.max(tag_probabilities, 1)\n",
    "        predictions = [ix_to_tag[x] for x in max_indices.numpy()]\n",
    "        \n",
    "        print('Before training:')\n",
    "        print(' - initial probabilities: {}'.format(store_initial_probabilities.pop()))\n",
    "        print(' - sentence: {}'.format(' '.join(sentence)))\n",
    "        print(' - predicition: {}'.format(store_initial_predictions.pop()))\n",
    "        print('After training:')\n",
    "        print(' - final probabilities: {}'.format(tag_probabilities))\n",
    "        print(' - sentence: {}'.format(' '.join(sentence)))\n",
    "        print(' - prediction: {}'.format(predictions))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do the scores mean?\n",
    "* The scores are used to predict the parts-of-speech for each word in a sentence. \n",
    "* A corresponding list of of possible parts-of-speech is assigned to each word. \n",
    "* This list is the same for all data passed through the model. \n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let us take the sentence: The dog ate the apple.\n",
      "For the word \"The\" the list of possible parts-of-speech are: ['Determiner', 'Noun', 'Verb']\n"
     ]
    }
   ],
   "source": [
    "print('Let us take the sentence: {}'.format(' '.join(training_sentences[0])))\n",
    "print('For the word \"{}\" the list of possible parts-of-speech are: {}'.format(training_sentences[0][0], [x for x in ix_to_tag.values()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A score is given to each part-of-speech in the list.\n",
    "* The prediction is the part-of-speech with the highest score.\n",
    "* Correct prediction = `Determiner` and will have highest score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model\n",
    "Save the Pytorch model to disk. This model will be used in the deployment tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = os.path.join(os.getcwd(), 'models', 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), models_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(models_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMTagger(\n",
       "  (word_embeddings): Embedding(9, 6)\n",
       "  (lstm): LSTM(6, 6)\n",
       "  (hidden2tag): Linear(in_features=6, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a training example through the loaded model and make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: The dog ate the apple.\n",
      "parts-of-speach: ['Determiner', 'Noun', 'Verb', 'Determiner', 'Noun']\n"
     ]
    }
   ],
   "source": [
    "inputs = prepare_sequence(training_sentences[0], word_to_ix)\n",
    "tag_scores = model(inputs)\n",
    "tag_probabilities = tag_scores.exp()\n",
    "max_values, max_indices = torch.max(tag_probabilities, 1)\n",
    "predictions = [ix_to_tag[x] for x in max_indices.numpy()]\n",
    "print('sentence: {}'.format(' '.join(training_sentences[0])))\n",
    "print('parts-of-speach: {}'.format(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "1. [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
