{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Models\n",
    "This tutorial is closely based off of [Sequence Models and Long-Short Term Memory Networks](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html) tutorial on the Pytorch webpage. The original author is Robert Guthrie.\n",
    "\n",
    "In this tutorial, you will learn about LSTM neural networks and see an example of how they can be used to recognize parts of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1035fbed0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import jdc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import Image\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "### What is an LSTM?\n",
    "LSTM stands for Long Short-Term Memory. It is a type of recurrent neural network (RNN). RNNs are neural networks that can learn relationships within sequences, such as time series or sentences. A normal RNN has difficulty learning relationships when there is a large separation within the sequence. For example, consider the sequence: _The humidity is very high. Today it is going to_ _**rain**_. A regular RNN might not do a good job predicting the word **rain**. But, an LSTM could do a much better job with this prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/PythonWorkshop/intro-to-nlp-with-pytorch/master/images/lstm_flow.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network can learn sequences of information and make predictions based off of what it learns. It is a type of recurrent neural network. The LSTM cell has a state, which gets updated as the network trains. It is this state that allows the network to remember.\n",
    "\n",
    "Here is a basic diagram of an LSTM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/PythonWorkshop/intro-to-nlp-with-pytorch/master/images/lstm_inner_workings.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Components\n",
    "An LSTM replaces the recurrent model of an RNN with a module that uses a cell state. Gates, composed of sigmoid activation functions, decide what information to keep within the cell state and what information to remove. These gates go by the following names:\n",
    "* Forget Gate\n",
    "* Input Gate\n",
    "* Output Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forget Gate\n",
    "Here the network decides what information to remove. Continuing with our previous example, let's say the network receives as input low humidity, the network might want to then adjust the rain likelihood for today to low. The forget gate then removes the current state of rain likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Gate.\n",
    "The input gate decides what information to add to the cell state. In our example, it would replace the previous rain likelihood with the new likelihood for dry weather."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Gate\n",
    "This gate returns a filtered version of the cell state. For example, it might keep track of the humidities magnitude to help the network decide whether to predict a large storm or just light rain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Very Simple Example 1\n",
    "Before approaching the main example for this tutorial, let's see a very brief example of how to create an LSTM cell in Pytorch and pass information through it. One way to do this is using a **`for`** loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_1: tensor([[[0.1286]]], grad_fn=<StackBackward>)\n",
      "h_1: (tensor([[[0.1286]]], grad_fn=<StackBackward>), tensor([[[0.5273]]], grad_fn=<StackBackward>))\n",
      "\n",
      "x_2: tensor([[[0.1395]]], grad_fn=<StackBackward>)\n",
      "h_2: (tensor([[[0.1395]]], grad_fn=<StackBackward>), tensor([[[0.4831]]], grad_fn=<StackBackward>))\n",
      "\n",
      "x_3: tensor([[[0.1322]]], grad_fn=<StackBackward>)\n",
      "h_3: (tensor([[[0.1322]]], grad_fn=<StackBackward>), tensor([[[0.5156]]], grad_fn=<StackBackward>))\n",
      "\n",
      "x_4: tensor([[[0.1449]]], grad_fn=<StackBackward>)\n",
      "h_4: (tensor([[[0.1449]]], grad_fn=<StackBackward>), tensor([[[0.4218]]], grad_fn=<StackBackward>))\n",
      "\n",
      "x_5: tensor([[[0.1443]]], grad_fn=<StackBackward>)\n",
      "h_5: (tensor([[[0.1443]]], grad_fn=<StackBackward>), tensor([[[0.4398]]], grad_fn=<StackBackward>))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define LSTM architecture\n",
    "sequence_len = 5  # The length of the sequence\n",
    "input_size = 1  # Number of input features per time step\n",
    "hidden_size = 1  # Number of LSTM blocks per layer of the RNN\n",
    "batch_size = 1  # The batch size\n",
    "output_size = hidden_size  \n",
    "lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size)\n",
    "\n",
    "# Create fake inputs for the LSTM\n",
    "inputs = torch.randn(sequence_len, batch_size, input_size)\n",
    "\n",
    "# Initialize the hidden state and cell states.\n",
    "hidden_0 = torch.randn(1, batch_size, hidden_size)\n",
    "cell_0 = torch.randn(1, batch_size, hidden_size)\n",
    "\n",
    "# Step through the LSTM as it takes in the input sequence\n",
    "for i, in_value in enumerate(inputs):\n",
    "    # Step through the sequence one element at a time.\n",
    "    # After each time step, hidden contains the hidden state.\n",
    "    out, hidden_out = lstm(in_value.view(1, 1, -1), (hidden_0, cell_0))\n",
    "    print('x_{}: {}'.format(i+1, out))\n",
    "    print('h_{}: {}'.format(i+1, hidden_out))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, **`x`** is the output and **`h`** is the value of the hidden and cell states at each step in the sequence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Very Simple Example 2\n",
    "Instead of creating an LSTM using a **`for`** loop, we can use Pytorch's **`cat`** function to string together each layer of the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM architecture\n",
    "sequence_len = 5  # The length of the sequence\n",
    "input_size = 1  # Number of input features per time step\n",
    "hidden_size = 1  # Number of LSTM blocks per layer of the RNN\n",
    "batch_size = 1  # The batch size\n",
    "output_size = hidden_size  \n",
    "lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size)\n",
    "\n",
    "# Create the inputs for the LSTM\n",
    "inputs = [torch.randn(batch_size, input_size) for _ in range(sequence_len)]\n",
    "\n",
    "# Concatenate the inputs so that they are a tensor\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "\n",
    "# Initialize the hidden state and cell states.\n",
    "hidden_0 = torch.randn(1, batch_size, hidden_size)\n",
    "cell_0 = torch.randn(1, batch_size, hidden_size)\n",
    "\n",
    "out, hidden = lstm(inputs, (hidden_0, cell_0))  # out = all states, hidden = last state and last cell state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: tensor([[[-0.1616]],\n",
      "\n",
      "        [[-0.1378]],\n",
      "\n",
      "        [[ 0.0745]],\n",
      "\n",
      "        [[-0.2183]],\n",
      "\n",
      "        [[-0.2227]]], grad_fn=<StackBackward>)\n",
      "last hidden and cell states: (tensor([[[-0.2227]]], grad_fn=<StackBackward>), tensor([[[-0.9171]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "print('out: {}'.format(out))\n",
    "print('last hidden and cell states: {}'.format(hidden))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: An LSTM for Part-of-Speech Tagging\n",
    "In this example, a model will be created that can predict the part-of-speech for each word in a sentence. TODO: Show rolled out LSTM, labeling each word and part of speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data:\n",
    "Let's begin by creating a dataset for training. The dataset will consist of a list of sequences. Each sequence will contain two lists. The first list is a sentence split up into words. The second list contains grammar identifiers for each word in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    (\"The dog ate the apple.\".split(), [\"Determiner\", \"Noun\", \"Verb\", \"Determiner\", \"Noun\"]),\n",
    "    (\"Everybody read that book.\".split(), [\"Noun\", \"Verb\", \"Determiner\", \"Noun\"])\n",
    "]\n",
    "training_sentences = [training_data[x][0] for x in range(len(training_data))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Pytorch only understands numbers, we need to map strings, such as the words in the training set, to integers. The following lines of code create a dictionary with this mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_to_ix: {'that': 7, 'the': 3, 'The': 0, 'apple.': 4, 'ate': 2, 'read': 6, 'book.': 8, 'dog': 1, 'Everybody': 5}\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print('word_to_ix: {}'.format(word_to_ix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to map the parts-of-speech tags to integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tags to integers\n",
    "tag_to_ix = {\"Determiner\": 0, \"Noun\": 1, \"Verb\": 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A third dictionary is used to map the integers back to parts-of-speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integers to tags\n",
    "ix_to_tag = {0: \"Determiner\", 1: \"Noun\", 2: \"Verb\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "LEARNING_RATE = 0.1\n",
    "NUM_EPOCHS = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "We will define the model by creating a Python class object. This class will inherit the nn.Module class from Pytorch, which will allow us to easily use the neural network classes defined in Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`LSTMTagger`** class will take in four values, the embedding dimension, the number of hidden dimensions, the vocabulary size, and the size of the tag set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM: Inputs are embeddings, outputs are hidden states\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # Linear layer maps hidden space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a function to initialize the hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to LSTMTagger\n",
    "def init_hidden(self):\n",
    "    \"\"\"\n",
    "    Initialize the hidden state. The axes correspond to (num_layers, minibatch_size, hidden_dim).\n",
    "    \"\"\"\n",
    "    return (torch.zeros(1, 1, self.hidden_dim),\n",
    "            torch.zeros(1, 1, self.hidden_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a function to make a forward pass through the recurrent LSTM network. It will return the predict tag values given an input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to LSTMTagger\n",
    "def forward(self, sentence):\n",
    "    \"\"\"\n",
    "    Make a forward pass through the LSTM.\n",
    "    \n",
    "    :param sentence: The input sentence.\n",
    "    :type sentence: list\n",
    "    :return: A Tensor of tag scores.\n",
    "    :rtype: Tensor\n",
    "    \"\"\"\n",
    "    embeds = self.word_embeddings(sentence)\n",
    "    lstm_out, self.hidden = self.lstm(\n",
    "        embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "    tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "    tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "    return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function\n",
    "This helper function will be used to map either words or tags to integers, using the previously defined dictionaries (**`tag_to_ix`**, **`ix_to_tag`**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    \"\"\"\n",
    "    Convert words or tags to intigers and return a Pytorch tensor.\n",
    "    :param seq: Sequence of words.\n",
    "    :type seq: list\n",
    "    :param to_ix: Dictionary mapping words or tags to intigers.\n",
    "    :return: A Pytorch tensor of indices.\n",
    "    :rtype: Tensor\n",
    "    \"\"\"\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the LSTM Pytorch model using the hyperparameters defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function. In this case, we will be using a negative log likelihood function, which is useful in classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can illustrate negative log likelihood in the following diagram:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/PythonWorkshop/intro-to-nlp-with-pytorch/master/images/nll_loss.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the model using stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the model before any training has been done and store the scores to a **`list`**. We will then compare these scores with the scores after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_initial_probabilities = []\n",
    "store_initial_predictions = []\n",
    "with torch.no_grad():\n",
    "    for sentence in training_sentences:\n",
    "        inputs = prepare_sequence(sentence, word_to_ix)\n",
    "        tag_scores = model(inputs)\n",
    "        tag_probabilities = tag_scores.exp()\n",
    "        max_values, max_indices = torch.max(tag_probabilities, 1)\n",
    "        initial_prediction = [ix_to_tag[x] for x in max_indices.numpy()]\n",
    "        store_initial_predictions.append(initial_prediction)\n",
    "        store_initial_probabilities.append(tag_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    for sentence, tags in training_data:\n",
    "        # Set gradients equal to zero after each intance\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Initialize hidden state of LSTM after each intance\n",
    "        model.hidden = model.init_hidden()\n",
    "        \n",
    "        # Turn inputs into tensors of word indices\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "        \n",
    "        # Run forward pass\n",
    "        tag_scores = model(sentence_in)\n",
    "        \n",
    "        # Compute the loss, gradients, and update the parameters\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        \n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update model parameters\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has now finished training. Let's print out some statistics to show how well the model training performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:\n",
      " - initial probabilities: tensor([[0.4666, 0.2908, 0.2426],\n",
      "        [0.4649, 0.2811, 0.2540],\n",
      "        [0.4950, 0.2623, 0.2427],\n",
      "        [0.4749, 0.2906, 0.2345],\n",
      "        [0.4839, 0.2764, 0.2398]])\n",
      " - sentence: The dog ate the apple.\n",
      " - predicition: ['Determiner', 'Determiner', 'Determiner', 'Determiner', 'Determiner']\n",
      "After training:\n",
      " - final probabilities: tensor([[0.6031, 0.2518, 0.1451],\n",
      "        [0.0267, 0.9718, 0.0014],\n",
      "        [0.0592, 0.0161, 0.9246],\n",
      "        [0.9479, 0.0264, 0.0257],\n",
      "        [0.0104, 0.9842, 0.0054]])\n",
      " - sentence: The dog ate the apple.\n",
      " - prediction: ['Determiner', 'Noun', 'Verb', 'Determiner', 'Noun']\n",
      "\n",
      "Before training:\n",
      " - initial probabilities: tensor([[0.4878, 0.2757, 0.2366],\n",
      "        [0.5013, 0.2558, 0.2429],\n",
      "        [0.4658, 0.2717, 0.2625],\n",
      "        [0.4935, 0.2628, 0.2437]])\n",
      " - sentence: Everybody read that book.\n",
      " - predicition: ['Determiner', 'Determiner', 'Determiner', 'Determiner']\n",
      "After training:\n",
      " - final probabilities: tensor([[0.0037, 0.9917, 0.0046],\n",
      "        [0.0658, 0.0127, 0.9214],\n",
      "        [0.9636, 0.0201, 0.0163],\n",
      "        [0.0049, 0.9918, 0.0033]])\n",
      " - sentence: Everybody read that book.\n",
      " - prediction: ['Noun', 'Verb', 'Determiner', 'Noun']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out the scores after training the model\n",
    "store_initial_probabilities.reverse()\n",
    "store_initial_predictions.reverse()\n",
    "with torch.no_grad():\n",
    "    for sentence in training_sentences:\n",
    "        inputs = prepare_sequence(sentence, word_to_ix)\n",
    "        tag_scores = model(inputs)\n",
    "        tag_probabilities = tag_scores.exp()\n",
    "        max_values, max_indices = torch.max(tag_probabilities, 1)\n",
    "        predictions = [ix_to_tag[x] for x in max_indices.numpy()]\n",
    "        \n",
    "        print('Before training:')\n",
    "        print(' - initial probabilities: {}'.format(store_initial_probabilities.pop()))\n",
    "        print(' - sentence: {}'.format(' '.join(sentence)))\n",
    "        print(' - predicition: {}'.format(store_initial_predictions.pop()))\n",
    "        print('After training:')\n",
    "        print(' - final probabilities: {}'.format(tag_probabilities))\n",
    "        print(' - sentence: {}'.format(' '.join(sentence)))\n",
    "        print(' - prediction: {}'.format(predictions))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do the scores mean?\n",
    "The scores are used to predict the parts-of-speech for each word in a sentence. A corresponding list of of possible parts-of-speech is assigned to each word. This list is the same for all data passed through the model. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let us take the sentence: The dog ate the apple.\n",
      "For the word \"The\" the list of possible parts-of-speech are: ['Determiner', 'Noun', 'Verb']\n"
     ]
    }
   ],
   "source": [
    "print('Let us take the sentence: {}'.format(' '.join(training_sentences[0])))\n",
    "print('For the word \"{}\" the list of possible parts-of-speech are: {}'.format(training_sentences[0][0], [x for x in ix_to_tag.values()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model assigns a scores to each part-of-speech in the list. The prediction is then the part-of-speech with the highest score. If the model makes a correct prediction, then `Determiner` will have the highest score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model\n",
    "Save the Pytorch model to disk. This model will be used in the deployment tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
