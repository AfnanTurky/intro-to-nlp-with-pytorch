{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch-RNN-next-char-pred.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "ImjwR5LNPFmp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# RNN next character prediction\n",
        "\n",
        "We will look at using RNN's in PyTorch for the task of predicting the next character from observing previous characters. This is just a toy example but the hope is to understand the basics of the architectures and training procedures.\n",
        "\n",
        "We will only consider one input sequence and one output sequence:\n",
        "\n",
        "- **Input sequence:** hihell\n",
        "- **Output sequence:** ihello\n",
        "\n",
        "One way of thinking about how this works is that the machine learning algorithm (ML) first sees the character \"h\" and tries to guess what should follow:\n",
        "\n",
        "\"h\" → ML → \"?\"\n",
        "\n",
        "In the ideal case and if it hsa learned, the ML will output \"i\". If we do this for all the characters, in this exercise we would like to see the ML algorithm at the end to have this property:\n",
        "\n",
        "\"h\" → ML →\"i\" <br>\n",
        "\"i\"  → ML → \"h\"<br>\n",
        "\"h\" → ML → \"e\"<br>\n",
        "\"e\" → ML → \"l\"<br>\n",
        "\"l\"  → ML → \"l\"<br>\n",
        "\"l\"  → ML → \"o\"<br>\n",
        "\n",
        "\n",
        "It's imortant to clarify that this example demostrates *memorization* and not learning: the network has simply memorized to regurgitate what it has seen. To truly test if a network has learned and not just memorized, we should give the network *new* sequences that it hasn't seen before. \n",
        "\n",
        "\n",
        "This notebook is mostly lifted and modified from the excellent tutorials by Sung Kim:\n",
        "https://docs.google.com/presentation/d/17VUX7YXhMkJrqO5gNGh6EE5gzBpY-BF9IrfVKcFIb3A/edit#slide=id.g27c9a844e4_157_9"
      ]
    },
    {
      "metadata": {
        "id": "Zy5bLqu6QFY9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "b53404b4-bbc3-4e08-e74c-ba061403d8cc"
      },
      "cell_type": "code",
      "source": [
        "# needed to use pytorch in colab\n",
        "!pip install torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 519.5MB 23kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x587cc000 @  0x7faa45edf1c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WaQwdaqHPFml",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mV5M0_amPFmq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## RNN basics in PyTorch\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Uon6zrqWtpYH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Encoding characters with numbers\n",
        "\n",
        "We are trying to predict a single character from observing past characters.  But we have to first somehow represent characters as numbers since thats what neural nets understand. \n",
        "\n",
        "We could do somethig like assign a random number to each character.  For example, we could do something like assigning the character \n",
        "\n",
        "\"h\" → 34 <br>\n",
        "\"e\" → 12 <br>\n",
        "\"l\"  → 43 <br>\n",
        "\"o\"  → 9 <br>\n",
        "\n",
        "So everytime we see the character \"h\" we treat as if it were the digit 34,  or if we saw the character \"e\" we treat it as 12 and so on. The problem with this encoding scheme is that it implies a specific ordering. That is, because \"h\" has been assigned to 34 and \"e\" to 12, it implies that \"h\" is somehow bigger than \"e\" by 22 units. \n",
        "\n",
        "But we know that the characters are all important and there is no quantitiative measure of one character being bigger or more important than another (though some entropic measures may guide us as to how to efficiently order/code characters).\n",
        "\n",
        "A popular way to deal with encoding characters instead is to treat all characters as equally important and assign a one-hot encoding scheme. It is easiest to illustrate this:\n",
        "\n",
        "\n",
        "\"h\" → 1000 <br>\n",
        "\"e\" → 0100 <br>\n",
        "\"l\"  → 0010 <br>\n",
        "\"o\"  → 0001 <br>\n",
        "\n",
        "You can think of each of these mappings as a \"1-hot code\" represented as a 4 dimensional binary vector. These codes can be assigned in python as a list:"
      ]
    },
    {
      "metadata": {
        "id": "yFd2WVJqPFms",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# One hot encoding for each char in 'hello'\n",
        "h = [1, 0, 0, 0]\n",
        "e = [0, 1, 0, 0]\n",
        "l = [0, 0, 1, 0]\n",
        "o = [0, 0, 0, 1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9xMlCSdUt4KO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since our 1-one-hot vectors are only 4 dimensional, we need the `input_size` of the RNN to be 4. The `hidden_size` is the dimension of the hidden state. "
      ]
    },
    {
      "metadata": {
        "id": "r689k5GvPFmw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Elman RNN\n",
        "\n",
        "\n",
        "The `nn.RNN` model is the classic vanilla RNN AKA the Elman RNN. It computes the hidden state $h_{t}$ at time $t$ based on the current input $x_{t}$ and the previus hidden state $h_{(t-1)}$. \n",
        "\n",
        "$ h_{t} = \\text{tanh}(w_{ih}x_{t} + b_{ih} + w_{hh}h_{(t-1)} + b_{hh})$\n",
        "\n",
        "- $w_{ih}$ and $b_{ih}$ are the weights and biases associated with the input $x_{t}$ at time $t$\n",
        "- $w_{hh}$ and $b_{hh}$ are the weights and biases associated with the previous hidden state $h_{(t-1)}$\n",
        "\n",
        "Important parameters that need to be specified:\n",
        "\n",
        "- input_size: The number of expected features in the input `x`\n",
        "- hidden_size: The number of features in the hidden state `h`\n",
        "\n",
        "Once `nn.RNN` has been defined, it takes two inputs (`x` and initial state `h_0`) and returns two outputs (`output` set of states and final state`h_n`). The inputs are:\n",
        "\n",
        "- input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence.\n",
        "- h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. For shallow and unidirectional RNNs (the default), `num_layers = 1` and `num_directions = 1`\n",
        "\n",
        "The outputs are:\n",
        "\n",
        "- `output` of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_k) from the last layer of the RNN, for each k. For unidirectional RNN (the default), `num_directions = 1`. \n",
        "- `h_n` (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for `n = seq_len.` For shallow and unidirectional RNNs (the default), `num_layers = 1` and `num_directions = 1`\n",
        "\n",
        "For each element in the input sequence, each layer computes the following function:"
      ]
    },
    {
      "metadata": {
        "id": "1iwwB8hyPFmx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The RNN cell will take two sets of inputs:\n",
        "#  - inputs x with 4 features; we specify the number of features with input_size\n",
        "#  - hidden state h with 2 features; we specify the number of hidden features \n",
        "#    with hidden_size = 2\n",
        "     \n",
        "elman_rnn = nn.RNN(input_size=4, hidden_size=2, batch_first=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K_5eIgFpPFm1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The above line has instantiated a elman_rnn object for us to process *sequences* of data that takes state and input data. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "O8rMdTlBuJLk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Prep the initial hidden state tensor\n",
        "When we start the RNN, we need to select something for the initial hidden state $h_0$. Here lets pick something from a random normal distribution."
      ]
    },
    {
      "metadata": {
        "id": "OkXHj8K5PFm1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dce41093-8928-4430-a712-31a9636c825c"
      },
      "cell_type": "code",
      "source": [
        "# To make a 2 dimensional hidden state vector, we make the initial hidden state\n",
        "# h_0 with the tensor size specified as:\n",
        "#\n",
        "#     (num_layers * num_directions, batch_size, hidden_size) \n",
        "#\n",
        "# (swap if batch_first = True when RNN cell was created above)\n",
        "\n",
        "hidden = Variable(torch.randn(1, 1, 2))\n",
        "hidden.size()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "MqsdmtdRX4cc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Again, we stress that for the RNN cell that we created, the hidden state will have two features. And since we only have 1 hidden layer, and only batch size of 1 (we only have one sequence), we expect the hidden state vector to be a 1 x 1 x 2 tensor. Indeed,  `hidden.size()` above verifies that we have initialized the tensor with the correct shape. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "8W19i-akuu0H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Prep the initial input sequence character\n",
        "\n",
        "Now let's propogate an input character through the RNN cell. We will first convert our list of one-hot encoded characters to a PyTorch tensor. "
      ]
    },
    {
      "metadata": {
        "id": "Ukd6NxofPFm6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0786a18c-f176-402c-c722-7ede1da4b77d"
      },
      "cell_type": "code",
      "source": [
        "# Propagate input through RNN\n",
        "# Input: (batch, seq_len, input_size) when batch_first=True\n",
        "input_characters = Variable(torch.Tensor([h, e, l, l, o]))\n",
        "\n",
        "input_characters.size()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "JVYgDH3mPFm_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since our one-hot encoded vectors are 4 dimensional, and our \"hello\" sequence consists of 5 characters, we except the input tensor to have size 5 x 4. We see that using `input_characters.size()` that this is indeed the case. \n",
        "\n",
        "\n",
        "Note that our `inputs` tensor is missing the batch dimension. In this case, since we only have 1 sequence (so just one batch), the `inputs` tensor needs to have a size `[1, 5, 4]` - we can easily reshape the tensor using the `.view(1,5,4)` method. Or, we could just do `.view(1,5,-1)` where the -1 will take care of the left over dimensions for us.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "QBwS8bcfvANE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Inference with RNN (aka \"forward pass\"): one character at a time\n",
        "\n",
        "OK, lets **finally** take our initial hidden state and the first character encoded characters and pass it to the Elman RNN that we defined as `elman_rnn`:"
      ]
    },
    {
      "metadata": {
        "id": "0qB7-8-RNyeD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3646fd07-92b2-4859-9a6b-c634ba5d8bfa"
      },
      "cell_type": "code",
      "source": [
        "out, hidden = elman_rnn(input_characters[0,:].view(1,1,-1), hidden)\n",
        "print(\"Encoded character size:\", input_characters[0,:].size(), \n",
        "      \"\\nhidden size:\", hidden.size() ,\n",
        "      \"\\nout size:\", out.size())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded character size: torch.Size([4]) \n",
            "hidden size: torch.Size([1, 1, 2]) \n",
            "out size: torch.Size([1, 1, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wRutv7VaQK4j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's unpack what we did here\n",
        "\n",
        "\n",
        "- `elman_rnn` expects two sets of input tensors: the input character and a hidden state\n",
        "- Remember that our input character sequences have 5 characters total and each character is encoded as a 4 dimensional one-hot coded vector and we have stored it all in the 5x4 tensor `input_characters`\n",
        "- In this cell we are trying to only pass the *first* character. The first character can be accessed using \"slice\" indexing: `input_characters[0,:]` → this says take the first character (out of 5) and every feature of our one-hot encoded vector\n",
        "- When we slice this way, you'll notice that you will only get a 1D tensor that has 4 elements.\n",
        "- But the RNN expect a *sequential tensor* with the shape (seq_len, batch, input_size) \n",
        "- So we have to reshape this 1D tensor into 3D by introducing some dummy dimensions\n",
        "- This can be elegantly done with the `.view()` method.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Below we will iteratre through each 1-hot-encoded character and see the output and hidden sizes of the RNN outputs."
      ]
    },
    {
      "metadata": {
        "id": "lYNFSdOtPFnB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "e2b1edb4-dd75-45b0-f95e-2cee7aaa626e"
      },
      "cell_type": "code",
      "source": [
        "for i, one_hot_encoded_encoded in enumerate(input_characters):\n",
        "    encoded_character = one_hot_encoded_encoded.view(1, 1, -1)\n",
        "    # Input: (batch, seq_len, input_size) when batch_first=True\n",
        "    out, hidden = elman_rnn(encoded_character, hidden)\n",
        "    print(\"Character\", i, \"tensor sizes:\")\n",
        "    print(\"  encoded character size:\", encoded_character.size(), \n",
        "          \"\\n  hidden size:\", hidden.size() ,\n",
        "          \"\\n  out size:\", out.size())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Character 0 tensor sizes:\n",
            "  encoded character size: torch.Size([1, 1, 4]) \n",
            "  hidden size: torch.Size([1, 1, 2]) \n",
            "  out size: torch.Size([1, 1, 2])\n",
            "Character 1 tensor sizes:\n",
            "  encoded character size: torch.Size([1, 1, 4]) \n",
            "  hidden size: torch.Size([1, 1, 2]) \n",
            "  out size: torch.Size([1, 1, 2])\n",
            "Character 2 tensor sizes:\n",
            "  encoded character size: torch.Size([1, 1, 4]) \n",
            "  hidden size: torch.Size([1, 1, 2]) \n",
            "  out size: torch.Size([1, 1, 2])\n",
            "Character 3 tensor sizes:\n",
            "  encoded character size: torch.Size([1, 1, 4]) \n",
            "  hidden size: torch.Size([1, 1, 2]) \n",
            "  out size: torch.Size([1, 1, 2])\n",
            "Character 4 tensor sizes:\n",
            "  encoded character size: torch.Size([1, 1, 4]) \n",
            "  hidden size: torch.Size([1, 1, 2]) \n",
            "  out size: torch.Size([1, 1, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VRdi9QltZl_W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We see that for every character, the input tensor has been reshaped as a 4 dimensional 1-hot-encoded vector with tensor size 1x1x4. The RNN cell then takes produces *two* tensors, `out` and `hidden`. `hidden` is just the output of the RNN state for the next time step $h_{t+1}$. The out and hidden tensors have the same shape. This is because `out` is just a copy of the `hidden`. We can verify this by going through the RNN cell again: "
      ]
    },
    {
      "metadata": {
        "id": "X06k_L33ZQxH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "96d35018-144e-41d0-b3a3-4541f70ea118"
      },
      "cell_type": "code",
      "source": [
        "for i, one_hot_encoded_encoded in enumerate(input_characters):\n",
        "    encoded_character = one_hot_encoded_encoded.view(1, 1, -1)\n",
        "    # Input: (batch, seq_len, input_size) when batch_first=True\n",
        "    out, hidden = elman_rnn(encoded_character, hidden)\n",
        "    if torch.all(torch.eq(out, hidden)).item() == 1:\n",
        "      print(\"Character\", i, \"hidden and out RNN are equal\")\n",
        "    else:\n",
        "      print(\"Character\", i, \"hidden and out RNN are not equal\")\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Character 0 hidden and out RNN are equal\n",
            "Character 1 hidden and out RNN are equal\n",
            "Character 2 hidden and out RNN are equal\n",
            "Character 3 hidden and out RNN are equal\n",
            "Character 4 hidden and out RNN are equal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7ohXkVMnPFnF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Indeed! We see that the `hidden` and `out` tensors that the RNN cell returns are not only equal in shape but also in value. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "YFYxNT1KwcuW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Inference with RNN: one sequence at a time\n",
        "\n",
        "Instead of going throug the sequence individually with the for loop, we can go through the sequence in one shot:"
      ]
    },
    {
      "metadata": {
        "id": "xEEIrWMnPFnF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "af1b5ebd-d5e2-43b4-e0d6-62d817c32fa0"
      },
      "cell_type": "code",
      "source": [
        "input_characters = input_characters.view(1, 5, -1)\n",
        "out, hidden = elman_rnn(input_characters, hidden)\n",
        "print(\"sequence of encoded character size\",input_characters.size(), \n",
        "      \"\\nhidden size\", hidden.size(), \n",
        "      \"\\nout size\", out.size())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sequence of encoded character size torch.Size([1, 5, 4]) \n",
            "hidden size torch.Size([1, 1, 2]) \n",
            "out size torch.Size([1, 5, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h_oUoJKaPFnJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We again stress the distinction between the `out` and `hidden` outputs of the RNN. The distinction is that the `out` is the hidden states for every time step whereas `hidden` is the hidden state for just the last time step. So the last time step for `out` should be identical to `hidden`. Let's check it out!"
      ]
    },
    {
      "metadata": {
        "id": "9pPPfResPFnK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "605acd04-8f7f-446f-d215-0c9698ea7a23"
      },
      "cell_type": "code",
      "source": [
        "out[:,-1,:] # the last element in the *sequence* of outputs of the RNN"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.6522, 0.1723]], grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "afEtGcRUPFnO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea2ddc68-2fdb-4896-95c7-e28a81938aac"
      },
      "cell_type": "code",
      "source": [
        "hidden"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.6522, 0.1723]]], grad_fn=<ViewBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "aFGY2gf7v_zP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Yep, they both have the same values! We could have also checked their values are equal using the `torch.eq` method:"
      ]
    },
    {
      "metadata": {
        "id": "yKVSor3QPFnT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3fbe9e6a-d5e2-4c43-ccae-62d5b8f2067d"
      },
      "cell_type": "code",
      "source": [
        "torch.all(torch.eq(out[:,-1,:], hidden))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1, dtype=torch.uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "2KH5U4KHPFnZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Inference with RNN: iterating through multiple sequences\n",
        "\n",
        "Now lets try multiple sequencse so we have more than 1 batch. Here we will consider 3 sequences each with the same length: \"hello\", \"eolll\", and \"lleel\"."
      ]
    },
    {
      "metadata": {
        "id": "b-Da-6LAPFnZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2ee7a801-50c8-4428-bf26-805e25b5840e"
      },
      "cell_type": "code",
      "source": [
        "# One cell RNN input_dim (4) -> output_dim (2). sequence: 5, batch 3\n",
        "# 3 batches 'hello', 'eolll', 'lleel'\n",
        "# rank = (3, 5, 4)\n",
        "inputs = Variable(torch.Tensor([[h, e, l, l, o],\n",
        "                                [e, o, l, l, l],\n",
        "                                [l, l, e, e, l]]))\n",
        "\n",
        "inputs.size()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 5, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "kqwoJFH4xaPl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We see from `inputs.size()` that the `inputs` tensor has size 3x5x4. These three dimensions correspond to:\n",
        "- dim 1: the number of sequences, 3 in this case\n",
        "- dim 2: the length of each sequence (ie the number of elements/characters in each sequence).  We have 5 characters for each sequence\n",
        "- dim 3: the number of features used to represent each character. Because we are using a 1-hot encoding scheme to repreent each character and we only have 4 characters, the number of features is just 4.\n",
        "\n",
        "\n",
        "OK, now that we have our inputs tensor setup, we now need to initialize the hidden state as before. The big difference before is because we have **three** sequences instead of one like the previous examples, we need to create three hidden tensors. "
      ]
    },
    {
      "metadata": {
        "id": "vHxx72RLPFne",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "30a8d078-22f3-4a25-9ebe-f13b3ee4f162"
      },
      "cell_type": "code",
      "source": [
        "# hidden : (num_layers * num_directions, batch, hidden_size) whether batch_first=True or False\n",
        "hidden = Variable(torch.randn(1, 3, 2))\n",
        "hidden.size()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "Hq8j1_1szasT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In other words, we have created 3 different hidden states each have dimension 2. \n",
        "\n",
        "OK, now that we have our hidden states and inputs tensors setup, lets forward pass them to Elman RNN!!"
      ]
    },
    {
      "metadata": {
        "id": "yTUtdYmuPFng",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6904aa29-6882-4cf1-fd4f-de015dd282a1"
      },
      "cell_type": "code",
      "source": [
        "# Propagate input through RNN\n",
        "# Input: (batch, seq_len, input_size) when batch_first=True\n",
        "# B x S x I\n",
        "out, hidden = elman_rnn(inputs, hidden)\n",
        "print(\"batch input size\", inputs.size(), \"\\nout size\", out.size(), \"\\nhidden size\", hidden.size())\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch input size torch.Size([3, 5, 4]) \n",
            "out size torch.Size([3, 5, 2]) \n",
            "hidden size torch.Size([1, 3, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VFCnWAdAzvlf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's unpack the outputs of the Elman RNN a bit:\n",
        "- As discussed above, we have 3 sequences, each of length 5, and each element (ie, character) in the sequence has 4 features\n",
        "- Because we have 3 sequences, we expect the RNN to have 3 outputs. Indeed we see that the first dimension of the `out` tensor has 3 elements. \n",
        "- Similarly, because each sequence has 5 elements/characters, we expect the `out` tensor to have a corresponding hidden state for each of these characters. Indeed we seee that the second dimension of the `out` tensor has 5 elements.\n",
        "- **Finally!** Because we have designed our RNN to have hidden states that are two dimensional, we exepct two features for each element in each sequence. This is why we see that the third dimension of the `out` tensor is 2. \n",
        "\n",
        "OK, what about the `hidden` tensor?\n",
        "\n",
        "Remember that the `hidden` tensor that the RNN returns is just the *last* output of the hidden state from the last input character. Because we have 3 sequences, we expect to have 3 of these hidden states. And because we have 2 features in our hidden state, we execpt these 3 hidden states to have 2 dimensions. \n",
        "\n",
        "\n",
        "And as before, we expect that the *last* element in the `out` tensor should be equal to the `hidden` tensor for every sequence. Below we show that this is indeed the case:"
      ]
    },
    {
      "metadata": {
        "id": "AFgSJTtPPFnk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2a7dd45d-fb9e-4c9a-ffac-a676a71854d6"
      },
      "cell_type": "code",
      "source": [
        "out[:,-1,:] # note that we are picking the last element of output hidden states for all sequences and all features"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.6872,  0.1915],\n",
              "        [ 0.6870, -0.4625],\n",
              "        [ 0.7577, -0.0241]], grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "5tUZ1U-MPFnp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ea6f8dbc-bf25-44c8-d763-01eab911aeeb"
      },
      "cell_type": "code",
      "source": [
        "hidden"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.6872,  0.1915],\n",
              "         [ 0.6870, -0.4625],\n",
              "         [ 0.7577, -0.0241]]], grad_fn=<ViewBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "2rvv4YDf1X7E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As before, we can use the method `torch.eq` to check for equality between between these values:"
      ]
    },
    {
      "metadata": {
        "id": "6tzNvt_UPFnv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ff119636-61ff-4d70-f0a3-155ca50dfe76"
      },
      "cell_type": "code",
      "source": [
        "torch.eq(out[:,-1,:], hidden)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1, 1],\n",
              "         [1, 1],\n",
              "         [1, 1]]], dtype=torch.uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "-ccUr7zFPFn0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Everything checks out!\n",
        "\n",
        "we can also not have the first dim be the batch size:"
      ]
    },
    {
      "metadata": {
        "id": "x_M9Fw5KPFn0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e4166997-9724-420c-bca5-8c861b85397a"
      },
      "cell_type": "code",
      "source": [
        "# One cell RNN input_dim (4) -> output_dim (2)\n",
        "elman_rnn = nn.RNN(input_size=4, hidden_size=2)\n",
        "\n",
        "# The given dimensions dim0 and dim1 are swapped.\n",
        "inputs = inputs.transpose(dim0=0, dim1=1)\n",
        "# Propagate input through RNN\n",
        "# Input: (seq_len, batch_size, input_size) when batch_first=False (default)\n",
        "# S x B x I\n",
        "out, hidden = elman_rnn(inputs, hidden)\n",
        "print(\"batch input size\", inputs.size(), \"out size\", out.size())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch input size torch.Size([5, 3, 4]) out size torch.Size([5, 3, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H-kfoinTPFn2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Learning 1-batch sequence with RNN one element at a time\n",
        "\n",
        "Lets now apply RNN to *learn* a sequence. We will only consider one input sequence and one output sequence:\n",
        "\n",
        "- Input sequence: hihell\n",
        "- Output sequence: ihello\n",
        "\n",
        "We will design the 1-hot-encoding by first assigning an index to each character:\n",
        "- \"h\" -> 0\n",
        "- \"i\" -> 1\n",
        "- \"e\" -> 2\n",
        "- \"l\" -> 3\n",
        "- \"o\" -> 4\n",
        "\n",
        "So in other words we are living in a world that has only these 5 characters. "
      ]
    },
    {
      "metadata": {
        "id": "AftvYZLLPFn4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.manual_seed(777)  # reproducibility\n",
        "#            0    1    2    3    4\n",
        "idx2char = ['h', 'i', 'e', 'l', 'o']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MYYcvo8UPFn7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now define our sequence input sequence `x_data` \"hihell\"  and our output sequence `y_data` \"ihello\"\n",
        "\n",
        "We also convert our characters to one-hot-encoded vectors using a simple lookup table."
      ]
    },
    {
      "metadata": {
        "id": "jvIAxxesPFn8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Teach hihell -> ihello\n",
        "x_data = [0, 1, 0, 2, 3, 3]   # hihell\n",
        "y_data = [1, 0, 2, 3, 3, 4]   # ihello\n",
        "\n",
        "one_hot_lookup = [[1, 0, 0, 0, 0],  # 0\n",
        "                  [0, 1, 0, 0, 0],  # 1\n",
        "                  [0, 0, 1, 0, 0],  # 2\n",
        "                  [0, 0, 0, 1, 0],  # 3\n",
        "                  [0, 0, 0, 0, 1]]  # 4\n",
        "\n",
        "\n",
        "x_one_hot = [one_hot_lookup[x] for x in x_data]\n",
        "\n",
        "# As we have one batch of samples, we will change them to variables only once\n",
        "inputs = Variable(torch.Tensor(x_one_hot))\n",
        "labels = Variable(torch.LongTensor(y_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WPcunGxbPFn-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "30ddaa30-8352-4d98-e266-27b6fbf60405"
      },
      "cell_type": "code",
      "source": [
        "inputs.size(), labels.size()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([6, 5]), torch.Size([6]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "eVZFnLMDMcky",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The `inputs` tensor is size 6x5 because there are 6 characters and each character has 5 features. The outut `labels` tensor is just the character \"classes\" (ie, which character encodings) we want to predict.\n",
        "\n",
        "The RNN we are going to use for predicting the next character is going to use the hidden state to directly in its prediction. Normally this would be passed to another layer (like a fully connected layer or even another RNN) but in this example we are just going to  use it directly. The advantage of using the hidden state directly and not introducing additional layers is we limit the number of parameters we have to learn. The disadvantage of not using an additional layer is that we expect the hidden state to encode *both* the past histories that we have observed (it's primary function) *and* predict the next character. \n",
        "\n",
        "Regardless of the demands we are placing on the hidden state, because this is such an easy problem (there are only 5 characters and only 1 sequence the network has to memorize), we expect the hidden state to be able to do this.\n",
        "\n",
        "One issue to keep in mind though is because we are using the hidden state to directly predict the next character, this constrains us to have the size of the hidden state be the same as the number of classes in our outputs (5). Let's define the different parameters of the RNN below:"
      ]
    },
    {
      "metadata": {
        "id": "p2Fh9retPFoB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_classes = 5      # the number of possible classes we have (the labels tensors is between 0 and 4)\n",
        "input_size = 5       # one-hot encoded vector dimensions\n",
        "hidden_size = 5      # we use 5 dimensional hidden state vectors to directly predict the character\n",
        "batch_size = 1       # we have one sentence and so one batch size\n",
        "sequence_length = 1  # we have only one sequence and we will process the characters one by one\n",
        "num_layers = 1       # we will have a simple one hidden layer RNN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mnv74fZOOPmT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "OK, now we define our RNN class with the specific architecture that we want (as we usually do with PyTorch neural network models for training)"
      ]
    },
    {
      "metadata": {
        "id": "99rk_BLLPFoF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size=input_size,\n",
        "                          hidden_size=hidden_size, \n",
        "                          batch_first=True)\n",
        "\n",
        "    def forward(self, hidden, x):\n",
        "        # Reshape input to make sure the first dim is batch dimension\n",
        "        x = x.view(batch_size, sequence_length, input_size)\n",
        "\n",
        "        # Propagate input through RNN\n",
        "        #   Input:  (batch, seq_len, input_size)\n",
        "        #            since we only have 1 batch and are iterating a single \n",
        "        #            character at a time we execpt the input tensor to have \n",
        "        #            shape: 1 x 1 x 5\n",
        "        #   hidden: (num_layers * num_directions, batch, hidden_size)\n",
        "        #            we only have 1 hidden layer and the RNN is uniderectional\n",
        "        #            so the hidden tensor size should be 1 x 1 x 5              \n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "        return hidden, out.view(-1, num_classes)\n",
        "\n",
        "    def init_hidden(self):\n",
        "        # Initialize hidden and cell states\n",
        "        # (num_layers * num_directions, batch, hidden_size)\n",
        "        #  we only have 1 hidden layer and the RNN is uniderectional\n",
        "        #  so the hidden tensor size should be 1 x 1 x 5   \n",
        "        return Variable(torch.zeros(num_layers, batch_size, hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TOuTLcG-Osi-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we instantiate the model, define our loss criteron, and define the optimizer we want to use:"
      ]
    },
    {
      "metadata": {
        "id": "7H870MYhPFoH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Instantiate RNN model\n",
        "model = Model()\n",
        "\n",
        "# Set loss and optimizer function\n",
        "# CrossEntropyLoss = LogSoftmax + NLLLoss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TeJymCWsPCte",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Time to train the network!!!! \n",
        "\n",
        "We will loop through each epoch (100 of them), forward pass each individual character, then compute and accumulate the loss. Once the sequence is over, we compute the total loss and propagate the errors to update the network.  "
      ]
    },
    {
      "metadata": {
        "id": "rA3E9fbBPFoJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "6b2a41db-52bd-4138-dac6-5692caca93cc"
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    loss = 0\n",
        "    hidden = model.init_hidden()\n",
        "    \n",
        "    # iterate through each character and predict what\n",
        "    # the next character should be.\n",
        "    # using the true target label, compute and accumulate\n",
        "    # the loss\n",
        "    pred_string = \"\"\n",
        "    for input, label in zip(inputs, labels):\n",
        "        hidden, output = model(hidden, input)\n",
        "        val, idx = output.max(1) # remember that we are using the hidden state\n",
        "                                 # directly to make our prediction (and have \n",
        "                                 # reshaped appropriately in our Model class \n",
        "                                 # definition). We could also just as well use \n",
        "                                 # hidden state that we are returning as long\n",
        "                                 # as we reshape it right: \n",
        "                                 # hidden.view(-1, num_classes).max(1) \n",
        "        \n",
        "        pred_string += idx2char[idx.data[0]]\n",
        "        # accumulate the loss\n",
        "        loss += criterion(output, label.view(1))\n",
        "    \n",
        "    # ok we completed the sequence, lets backward prop and\n",
        "    # update the network\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # print every 20 epochs what the results look like\n",
        "    if (epoch%20 == 0) or (epoch == 99):\n",
        "      sys.stdout.write(\"predicted string: \")\n",
        "      sys.stdout.write(pred_string)\n",
        "      print(\", epoch: %d, loss: %1.3f\" % (epoch + 1, loss.item()))    \n",
        "    \n",
        "print(\"Learning finished!\")    "
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted string: ihello, epoch: 1, loss: 2.741\n",
            "predicted string: ihello, epoch: 21, loss: 2.734\n",
            "predicted string: ihello, epoch: 41, loss: 2.728\n",
            "predicted string: ihello, epoch: 61, loss: 2.722\n",
            "predicted string: ihello, epoch: 81, loss: 2.718\n",
            "predicted string: ihello, epoch: 100, loss: 2.713\n",
            "Learning finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8gYILUvISsA6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Looks like we learned (memorized, really) the target sequence!!!\n",
        "\n",
        "We can generalize this approach to multiple sequences: we would just have one more loop that would iterate through each sequence. All of our sequences could also have different lengths and it would not matter.\n",
        "\n",
        "But often we are faced with learning sequences that always have the same length. In those cases it would be tedious and slow to iterate through yet another for loop. This is where we can update our RNN model to process not just a single character at a time, but the entire sequence. "
      ]
    },
    {
      "metadata": {
        "id": "5cBl0Me3PFoO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Learning 1-batch sequence with RNN (entire sequence)\n",
        "\n",
        "Now we are going to learn the sequence not character-by-character but the entire sequence at one. Another way of thinking about this is that we are learning batches of sequences. But since we onlyu have 1 sequence, our batch size will be 1. "
      ]
    },
    {
      "metadata": {
        "id": "go2igLFrPFoQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "sequence_length = 6  # Since the number of character in our sequence |ihello| == 6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TDvzUv2NUC3Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will similarly define the parameters of our model as variables below:"
      ]
    },
    {
      "metadata": {
        "id": "U-gDSCPVnZfn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_classes = 5      # the number of possible classes we have (the labels tensors is between 0 and 4)\n",
        "input_size = 5       # one-hot encoded vector dimensions\n",
        "hidden_size = 5      # we use 5 dimensional hidden state vectors to directly predict the character\n",
        "batch_size = 1       # we have one sentence and so one batch size\n",
        "sequence_length = 1  # we have only one sequence and we will process the characters one by one\n",
        "num_layers = 1       # we will have a simple one hidden layer RNN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d9AsRC5VUIP4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we define our RNN class with the specific architecture that we want (as we usually do with PyTorch neural network models for training). "
      ]
    },
    {
      "metadata": {
        "id": "f-IIeOBIPFoV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.num_layers = num_layers\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "        self.rnn = nn.RNN(input_size=5, hidden_size=5, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden and cell states\n",
        "        # (num_layers * num_directions, batch, hidden_size) for batch_first=True\n",
        "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
        "\n",
        "\n",
        "        # Propagate input through RNN\n",
        "        # Input: (batch, seq_len, input_size)\n",
        "        # h_0: (num_layers * num_directions, batch, hidden_size)\n",
        "\n",
        "        out, _ = self.rnn(x, h_0)\n",
        "        #return out\n",
        "        return out.view(-1, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tf7QfD4qp-EV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# As we have one batch of samples, we will change them to variables only once\n",
        "inputs = Variable(torch.Tensor(x_one_hot))\n",
        "labels = Variable(torch.LongTensor(y_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W6pdQ7kdp32K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e87be465-3da5-409f-ef34-2dd7c2eab86b"
      },
      "cell_type": "code",
      "source": [
        "inputs.size()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "dls8lStsPFoY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Instantiate RNN model\n",
        "rnn = RNN(num_classes, input_size, hidden_size, num_layers)\n",
        "\n",
        "# Set loss and optimizer function\n",
        "# CrossEntropyLoss = LogSoftmax + NLLLoss\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ia8GmMSQpfe8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "outputs = rnn(inputs.view(1,6,-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iCu4AnarphM3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a2eaaf44-0312-4b67-8fe8-da3078a480e5"
      },
      "cell_type": "code",
      "source": [
        "outputs.size()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "qcFLvMZlqFbp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "597983ac-811c-4ae8-ae39-3ccc58ddecea"
      },
      "cell_type": "code",
      "source": [
        "labels.size()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "G_Kab9RePFoZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "f4e107d9-70af-4b27-dba4-1a8c75289df0"
      },
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "for epoch in range(100):\n",
        "    outputs = rnn(inputs.view(1,6,-1))\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    _, idx = outputs.max(1)\n",
        "    idx = idx.data.numpy()\n",
        "    result_str = [idx2char[c] for c in idx.squeeze()]\n",
        "    if (epoch%20 == 0) or (epoch == 99):\n",
        "      print(\"epoch: %d, loss: %1.3f\" % (epoch + 1, loss.item()))\n",
        "      print(\"Predicted string: \", ''.join(result_str))\n",
        "      \n",
        "\n",
        "print(\"Learning finished!\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1, loss: 1.662\n",
            "Predicted string:  hohhhh\n",
            "epoch: 21, loss: 0.629\n",
            "Predicted string:  ihello\n",
            "epoch: 41, loss: 0.497\n",
            "Predicted string:  ihello\n",
            "epoch: 61, loss: 0.481\n",
            "Predicted string:  ihello\n",
            "epoch: 81, loss: 0.475\n",
            "Predicted string:  ihello\n",
            "epoch: 100, loss: 0.471\n",
            "Predicted string:  ihello\n",
            "Learning finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "32JwvNzkPFoc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## RNN with Embedding and Output layers\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "-XiFI-LlPFoe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_data = [[0, 1, 0, 2, 3, 3]] \n",
        "# As we have one batch of samples, we will change them to variables only once\n",
        "inputs = Variable(torch.LongTensor(x_data))\n",
        "labels = Variable(torch.LongTensor(y_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-1KjuASk7o-o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_size = 10  # embedding size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mZSTBc9_PFoi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size):    \n",
        "        super(Model, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.RNN(input_size=embedding_size,\n",
        "                          hidden_size=self.hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden and cell states\n",
        "        # (num_layers * num_directions, batch, hidden_size)\n",
        "        #h_0 = Variable(torch.zeros(1, embedding_size, self.hidden_size))\n",
        "        h_0 = Variable(torch.zeros(1, 1, self.hidden_size))\n",
        "\n",
        "        emb = self.embedding(x)\n",
        "        emb = emb.view(batch_size, embedding_size, -1)\n",
        "        # Propagate embedding through RNN\n",
        "        # Input: (batch, seq_len, embedding_size)\n",
        "        # h_0: (num_layers * num_directions, batch, hidden_size)\n",
        "        out, _ = self.rnn(emb.view(1,6,-1), h_0)\n",
        "        return self.fc(out)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bTcqAwMSPFok",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "7663599f-9817-4a3e-b40e-9de8da5efdc0"
      },
      "cell_type": "code",
      "source": [
        "# Instantiate RNN model\n",
        "model = Model(hidden_size)\n",
        "print(model)\n",
        "\n",
        "# Set loss and optimizer function\n",
        "# CrossEntropyLoss = LogSoftmax + NLLLoss\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model(\n",
            "  (embedding): Embedding(5, 10)\n",
            "  (rnn): RNN(10, 5, batch_first=True)\n",
            "  (fc): Linear(in_features=5, out_features=5, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EEc-GWKfPFoo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "925db20e-bab9-49bb-fd21-e49f4f0341b2"
      },
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "for epoch in range(100):\n",
        "    outputs = model(inputs.view(1,-1)).view(-1,num_classes)\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    _, idx = outputs.max(1)\n",
        "    idx = idx.data.numpy()\n",
        "    result_str = [idx2char[c] for c in idx.squeeze()]\n",
        "    if (epoch%20 == 0) or (epoch == 99):\n",
        "      print(\"epoch: %d, loss: %1.3f\" % (epoch + 1, loss.data[0]))\n",
        "      print(\"Predicted string: \", ''.join(result_str))\n",
        "\n",
        "print(\"Learning finished!\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 1, loss: 0.001\n",
            "Predicted string:  ihello\n",
            "epoch: 21, loss: 0.001\n",
            "Predicted string:  ihello\n",
            "epoch: 41, loss: 0.001\n",
            "Predicted string:  ihello\n",
            "epoch: 61, loss: 0.001\n",
            "Predicted string:  ihello\n",
            "epoch: 81, loss: 0.001\n",
            "Predicted string:  ihello\n",
            "epoch: 100, loss: 0.001\n",
            "Predicted string:  ihello\n",
            "Learning finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2LeCZ8Cwqzb8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}